"""
Critic Agent - ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng v√† t√≠nh ƒë√∫ng ƒë·∫Øn c·ªßa k·∫øt qu·∫£
Ph·∫£n bi·ªán v√† ƒë·ªÅ xu·∫•t c·∫£i thi·ªán cho c√°c response t·ª´ agents kh√°c
"""

from typing import Dict, List, Optional, Any
import json
import logging
from .base import BaseAgent

logger = logging.getLogger(__name__)


class CriticAgent(BaseAgent):
    """Agent chuy√™n tr√°ch ƒë√°nh gi√° v√† ph·∫£n bi·ªán k·∫øt qu·∫£ t·ª´ c√°c agents kh√°c"""
    
    def __init__(self):
        super().__init__("Critic", "critic.md")
        self.evaluation_criteria = {
            "accuracy": "T√≠nh ch√≠nh x√°c c·ªßa th√¥ng tin",
            "completeness": "T√≠nh ƒë·∫ßy ƒë·ªß c·ªßa c√¢u tr·∫£ l·ªùi", 
            "relevance": "M·ª©c ƒë·ªô li√™n quan ƒë·∫øn y√™u c·∫ßu",
            "clarity": "T√≠nh r√µ r√†ng v√† d·ªÖ hi·ªÉu",
            "actionability": "Kh·∫£ nƒÉng th·ª±c hi·ªán/h√†nh ƒë·ªông",
            "safety": "T√≠nh an to√†n v√† tu√¢n th·ªß quy ƒë·ªãnh"
        }
    
    def process(self, user_message: str, chat_history: List[Dict], context: Dict = None) -> Dict:
        """
        ƒê√°nh gi√° response t·ª´ agents kh√°c
        Context ph·∫£i ch·ª©a response c·∫ßn ƒë√°nh gi√°
        """
        try:
            if not context or "response_to_evaluate" not in context:
                return self._create_error_response("Kh√¥ng c√≥ response ƒë·ªÉ ƒë√°nh gi√°")
            
            response_to_evaluate = context["response_to_evaluate"]
            original_request = context.get("original_request", user_message)
            
            # Th·ª±c hi·ªán ƒë√°nh gi√°
            evaluation_result = self._evaluate_response(
                original_request, response_to_evaluate, context
            )
            
            # T·∫°o feedback v√† ƒë·ªÅ xu·∫•t c·∫£i thi·ªán
            improvement_suggestions = self._generate_improvement_suggestions(
                evaluation_result, response_to_evaluate, original_request
            )
            
            return {
                "reply": self._format_evaluation_summary(evaluation_result, improvement_suggestions),
                "agent": "critic",
                "evaluation": evaluation_result,
                "improvement_suggestions": improvement_suggestions,
                "overall_quality": evaluation_result["overall_score"]
            }
            
        except Exception as e:
            logger.exception("Error in CriticAgent.process")
            return self._create_error_response(str(e))
    
    def _evaluate_response(self, original_request: str, response: Dict, context: Dict) -> Dict:
        """ƒê√°nh gi√° chi ti·∫øt response theo c√°c ti√™u ch√≠"""
        
        evaluation_prompt = f"""
        ƒê√°nh gi√° response sau theo c√°c ti√™u ch√≠ ch·∫•t l∆∞·ª£ng:
        
        Y√äU C·∫¶U G·ªêC: {original_request}
        
        RESPONSE C·∫¶N ƒê√ÅNH GI√Å:
        {json.dumps(response, ensure_ascii=False, indent=2)}
        
        CONTEXT B·ªî SUNG:
        {json.dumps(context, ensure_ascii=False, indent=2)}
        
        TI√äU CH√ç ƒê√ÅNH GI√Å:
        {json.dumps(self.evaluation_criteria, ensure_ascii=False, indent=2)}
        
        H√£y ƒë√°nh gi√° theo t·ª´ng ti√™u ch√≠ v√† tr·∫£ v·ªÅ JSON:
        {{
            "scores": {{
                "accuracy": 8.5,
                "completeness": 7.0,
                "relevance": 9.0,
                "clarity": 8.0,
                "actionability": 6.5,
                "safety": 9.5
            }},
            "overall_score": 8.1,
            "strengths": [
                "C√¢u tr·∫£ l·ªùi ch√≠nh x√°c v√† ph√π h·ª£p",
                "Th√¥ng tin ƒë·∫ßy ƒë·ªß v√† chi ti·∫øt"
            ],
            "weaknesses": [
                "Thi·∫øu h∆∞·ªõng d·∫´n c·ª• th·ªÉ cho vi·ªác th·ª±c hi·ªán",
                "C√≥ th·ªÉ r√∫t g·ªçn ƒë·ªÉ d·ªÖ hi·ªÉu h∆°n"
            ],
            "critical_issues": [
                "Kh√¥ng c√≥ v·∫•n ƒë·ªÅ nghi√™m tr·ªçng n√†o"
            ]
        }}
        
        Thang ƒëi·ªÉm: 0-10 (10 l√† t·ªët nh·∫•t)
        """
        
        messages = [{"role": "user", "content": evaluation_prompt}]
        response_text = self._call_llm(messages)
        
        try:
            return json.loads(response_text)
        except json.JSONDecodeError:
            logger.warning("Failed to parse evaluation result")
            return self._create_fallback_evaluation(response)
    
    def _create_fallback_evaluation(self, response: Dict) -> Dict:
        """T·∫°o ƒë√°nh gi√° fallback khi LLM kh√¥ng tr·∫£ v·ªÅ JSON h·ª£p l·ªá"""
        
        # ƒê√°nh gi√° c∆° b·∫£n d·ª±a tr√™n structure c·ªßa response
        scores = {}
        
        # Accuracy: C√≥ th√¥ng tin c·ª• th·ªÉ kh√¥ng?
        scores["accuracy"] = 7.0 if response.get("reply") and len(response["reply"]) > 50 else 5.0
        
        # Completeness: Response c√≥ ƒë·∫ßy ƒë·ªß th√¥ng tin kh√¥ng?
        scores["completeness"] = 7.0 if "agent" in response and "reply" in response else 5.0
        
        # Relevance: M·∫∑c ƒë·ªãnh trung b√¨nh
        scores["relevance"] = 6.5
        
        # Clarity: D·ª±a tr√™n ƒë·ªô d√†i v√† c·∫•u tr√∫c
        reply_length = len(response.get("reply", ""))
        scores["clarity"] = 8.0 if 50 <= reply_length <= 500 else 6.0
        
        # Actionability: C√≥ suggested_action kh√¥ng?
        scores["actionability"] = 8.0 if "suggested_action" in response else 5.0
        
        # Safety: M·∫∑c ƒë·ªãnh cao (kh√¥ng c√≥ n·ªôi dung nh·∫°y c·∫£m)
        scores["safety"] = 9.0
        
        overall_score = sum(scores.values()) / len(scores)
        
        return {
            "scores": scores,
            "overall_score": round(overall_score, 1),
            "strengths": ["Response c√≥ c·∫•u tr√∫c c∆° b·∫£n"],
            "weaknesses": ["Kh√¥ng th·ªÉ ƒë√°nh gi√° chi ti·∫øt do l·ªói parse"],
            "critical_issues": []
        }
    
    def _generate_improvement_suggestions(self, evaluation: Dict, response: Dict, original_request: str) -> List[Dict]:
        """T·∫°o ƒë·ªÅ xu·∫•t c·∫£i thi·ªán d·ª±a tr√™n k·∫øt qu·∫£ ƒë√°nh gi√°"""
        
        suggestions = []
        scores = evaluation.get("scores", {})
        
        # ƒê·ªÅ xu·∫•t c·∫£i thi·ªán cho t·ª´ng ti√™u ch√≠ ƒëi·ªÉm th·∫•p
        for criterion, score in scores.items():
            if score < 7.0:  # Threshold cho ƒëi·ªÉm th·∫•p
                suggestion = self._get_improvement_suggestion(criterion, score, response, original_request)
                if suggestion:
                    suggestions.append(suggestion)
        
        # ƒê·ªÅ xu·∫•t t·ª´ critical issues
        critical_issues = evaluation.get("critical_issues", [])
        for issue in critical_issues:
            if issue != "Kh√¥ng c√≥ v·∫•n ƒë·ªÅ nghi√™m tr·ªçng n√†o":
                suggestions.append({
                    "type": "critical_fix",
                    "priority": "high",
                    "description": f"Kh·∫Øc ph·ª•c v·∫•n ƒë·ªÅ nghi√™m tr·ªçng: {issue}",
                    "specific_action": self._get_critical_fix_action(issue)
                })
        
        return suggestions
    
    def _get_improvement_suggestion(self, criterion: str, score: float, response: Dict, original_request: str) -> Optional[Dict]:
        """T·∫°o ƒë·ªÅ xu·∫•t c·∫£i thi·ªán cho m·ªôt ti√™u ch√≠ c·ª• th·ªÉ"""
        
        suggestion_map = {
            "accuracy": {
                "description": "C·∫£i thi·ªán t√≠nh ch√≠nh x√°c th√¥ng tin",
                "specific_action": "Ki·ªÉm tra l·∫°i th√¥ng tin v·ªõi ngu·ªìn ƒë√°ng tin c·∫≠y v√† b·ªï sung chi ti·∫øt c·ª• th·ªÉ",
                "priority": "high" if score < 5.0 else "medium"
            },
            "completeness": {
                "description": "B·ªï sung th√¥ng tin thi·∫øu s√≥t",
                "specific_action": "Th√™m th√¥ng tin v·ªÅ quy tr√¨nh, th·ªùi gian, v√† c√°c y√™u c·∫ßu c·∫ßn thi·∫øt",
                "priority": "medium"
            },
            "relevance": {
                "description": "TƒÉng m·ª©c ƒë·ªô li√™n quan ƒë·∫øn y√™u c·∫ßu",
                "specific_action": "T·∫≠p trung v√†o c√°c kh√≠a c·∫°nh tr·ª±c ti·∫øp li√™n quan ƒë·∫øn c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng",
                "priority": "high"
            },
            "clarity": {
                "description": "C·∫£i thi·ªán t√≠nh r√µ r√†ng v√† d·ªÖ hi·ªÉu",
                "specific_action": "S·ª≠ d·ª•ng ng√¥n ng·ªØ ƒë∆°n gi·∫£n h∆°n v√† c·∫•u tr√∫c c√¢u tr·∫£ l·ªùi c√≥ logic",
                "priority": "medium"
            },
            "actionability": {
                "description": "TƒÉng kh·∫£ nƒÉng th·ª±c hi·ªán h√†nh ƒë·ªông",
                "specific_action": "Cung c·∫•p c√°c b∆∞·ªõc th·ª±c hi·ªán c·ª• th·ªÉ v√† r√µ r√†ng",
                "priority": "high" if "technical" in response.get("agent", "") else "medium"
            },
            "safety": {
                "description": "ƒê·∫£m b·∫£o t√≠nh an to√†n v√† tu√¢n th·ªß quy ƒë·ªãnh",
                "specific_action": "Ki·ªÉm tra l·∫°i c√°c th√¥ng tin nh·∫°y c·∫£m v√† tu√¢n th·ªß ch√≠nh s√°ch b·∫£o m·∫≠t",
                "priority": "high"
            }
        }
        
        if criterion in suggestion_map:
            suggestion_info = suggestion_map[criterion]
            return {
                "type": "improvement",
                "criterion": criterion,
                "current_score": score,
                "priority": suggestion_info["priority"],
                "description": suggestion_info["description"],
                "specific_action": suggestion_info["specific_action"]
            }
        
        return None
    
    def _get_critical_fix_action(self, issue: str) -> str:
        """T·∫°o h√†nh ƒë·ªông kh·∫Øc ph·ª•c cho v·∫•n ƒë·ªÅ nghi√™m tr·ªçng"""
        
        if "b·∫£o m·∫≠t" in issue.lower() or "security" in issue.lower():
            return "Xem x√©t l·∫°i v√† lo·∫°i b·ªè th√¥ng tin nh·∫°y c·∫£m, √°p d·ª•ng c√°c bi·ªán ph√°p b·∫£o m·∫≠t ph√π h·ª£p"
        elif "sai l·ªách" in issue.lower() or "incorrect" in issue.lower():
            return "Ki·ªÉm tra l·∫°i th√¥ng tin v·ªõi ngu·ªìn ch√≠nh th·ª©c v√† s·ª≠a ƒë·ªïi n·ªôi dung sai l·ªách"
        elif "ch√≠nh s√°ch" in issue.lower() or "policy" in issue.lower():
            return "ƒê·∫£m b·∫£o tu√¢n th·ªß ƒë√∫ng ch√≠nh s√°ch v√† quy ƒë·ªãnh c·ªßa tr∆∞·ªùng"
        else:
            return f"Kh·∫Øc ph·ª•c ngay v·∫•n ƒë·ªÅ: {issue}"
    
    def _format_evaluation_summary(self, evaluation: Dict, suggestions: List[Dict]) -> str:
        """T·∫°o t√≥m t·∫Øt ƒë√°nh gi√° d·ªÖ ƒë·ªçc"""
        
        overall_score = evaluation.get("overall_score", 0)
        scores = evaluation.get("scores", {})
        strengths = evaluation.get("strengths", [])
        weaknesses = evaluation.get("weaknesses", [])
        
        # X√°c ƒë·ªãnh m·ª©c ƒë·ªô ch·∫•t l∆∞·ª£ng
        if overall_score >= 8.5:
            quality_level = "Xu·∫•t s·∫Øc"
        elif overall_score >= 7.0:
            quality_level = "T·ªët"
        elif overall_score >= 5.5:
            quality_level = "Trung b√¨nh"
        else:
            quality_level = "C·∫ßn c·∫£i thi·ªán"
        
        summary = f"**ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng response: {quality_level} ({overall_score}/10)**\n\n"
        
        # Chi ti·∫øt ƒëi·ªÉm s·ªë
        summary += "**Chi ti·∫øt ƒëi·ªÉm s·ªë:**\n"
        for criterion, score in scores.items():
            criterion_name = self.evaluation_criteria.get(criterion, criterion)
            summary += f"- {criterion_name}: {score}/10\n"
        
        # ƒêi·ªÉm m·∫°nh
        if strengths:
            summary += f"\n**ƒêi·ªÉm m·∫°nh:**\n"
            for strength in strengths:
                summary += f"‚úì {strength}\n"
        
        # ƒêi·ªÉm y·∫øu
        if weaknesses:
            summary += f"\n**ƒêi·ªÉm c·∫ßn c·∫£i thi·ªán:**\n"
            for weakness in weaknesses:
                summary += f"‚ö† {weakness}\n"
        
        # ƒê·ªÅ xu·∫•t c·∫£i thi·ªán
        high_priority_suggestions = [s for s in suggestions if s.get("priority") == "high"]
        if high_priority_suggestions:
            summary += f"\n**ƒê·ªÅ xu·∫•t ∆∞u ti√™n cao:**\n"
            for suggestion in high_priority_suggestions:
                summary += f"üî• {suggestion['description']}: {suggestion['specific_action']}\n"
        
        return summary
    
    def _create_error_response(self, error_message: str) -> Dict:
        """T·∫°o response khi g·∫∑p l·ªói"""
        return {
            "reply": "Kh√¥ng th·ªÉ th·ª±c hi·ªán ƒë√°nh gi√° do l·ªói h·ªá th·ªëng.",
            "agent": "critic",
            "error": error_message,
            "evaluation": None
        }
    
    def evaluate_workflow_result(self, workflow_plan: Dict, execution_results: List[Dict], 
                                final_response: str) -> Dict:
        """ƒê√°nh gi√° k·∫øt qu·∫£ c·ªßa to√†n b·ªô workflow"""
        
        evaluation_prompt = f"""
        ƒê√°nh gi√° k·∫øt qu·∫£ workflow ƒë√£ th·ª±c hi·ªán:
        
        K·∫æ HO·∫†CH WORKFLOW:
        {json.dumps(workflow_plan, ensure_ascii=False, indent=2)}
        
        K·∫æT QU·∫¢ TH·ª∞C HI·ªÜN:
        {json.dumps(execution_results, ensure_ascii=False, indent=2)}
        
        RESPONSE CU·ªêI C√ôNG:
        {final_response}
        
        ƒê√°nh gi√°:
        - Workflow c√≥ ƒë∆∞·ª£c th·ª±c hi·ªán ƒë√∫ng k·∫ø ho·∫°ch kh√¥ng?
        - K·∫øt qu·∫£ c√≥ ƒë√°p ·ª©ng m·ª•c ti√™u ban ƒë·∫ßu kh√¥ng?
        - C√≥ b∆∞·ªõc n√†o th·∫•t b·∫°i ho·∫∑c kh√¥ng c·∫ßn thi·∫øt kh√¥ng?
        - Hi·ªáu qu·∫£ t·ªïng th·ªÉ nh∆∞ th·∫ø n√†o?
        
        Tr·∫£ v·ªÅ JSON:
        {{
            "workflow_success": true/false,
            "plan_adherence": 0.9,
            "goal_achievement": 0.8,
            "efficiency_score": 0.7,
            "failed_steps": ["step_id_1"],
            "unnecessary_steps": ["step_id_2"],
            "missing_steps": ["b∆∞·ªõc_thi·∫øu"],
            "overall_assessment": "T·ªïng quan ƒë√°nh gi√°",
            "recommendations": [
                "ƒê·ªÅ xu·∫•t c·∫£i thi·ªán cho l·∫ßn sau"
            ]
        }}
        """
        
        messages = [{"role": "user", "content": evaluation_prompt}]
        response = self._call_llm(messages)
        
        try:
            return json.loads(response)
        except json.JSONDecodeError:
            return {
                "workflow_success": True,
                "plan_adherence": 0.5,
                "goal_achievement": 0.5,
                "efficiency_score": 0.5,
                "overall_assessment": "Kh√¥ng th·ªÉ ƒë√°nh gi√° chi ti·∫øt do l·ªói parse",
                "recommendations": ["C·∫ßn c·∫£i thi·ªán h·ªá th·ªëng ƒë√°nh gi√°"]
            }
